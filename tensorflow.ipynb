{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHYNkPHDoraZ+OrlGzh8IQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitalibodke/DevOpsForAI/blob/master/tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ0S20iuePUc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "ij_n5IRzeP2j",
        "outputId": "a0bfd768-74e6-4889-9c39-2522b6fe151d"
      },
      "source": [
        "#IMPORT EXTERNAL LIBRARIES\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from time import sleep\n",
        "from random import randint\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "#### ADD #####\n",
        "# IMPORT AGENT LIB\n",
        "from MLWatcher.agent import MonitoringAgent\n",
        "\n",
        "WORKING_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "MODEL_DIR = os.path.join(WORKING_DIR, \"MODEL_MULTICLASS\")\n",
        "MODEL_W_DIR = os.path.join(WORKING_DIR, \"MODEL_MULTICLASS_WRONG\")\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "        os.mkdir(MODEL_DIR)\n",
        "if not os.path.exists(MODEL_W_DIR):\n",
        "        os.mkdir(MODEL_W_DIR)\n",
        "\n",
        "##EDITABLE##\n",
        "TRAIN_PHASE = True\n",
        "WRONG_TRAIN = False\n",
        "##END EDITABLE##\n",
        "\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "NUM_FEATURES = 784\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "TEST_BATCH_SIZE = 15\n",
        "\n",
        "\n",
        "def weight_variable(shape, name):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def bias_variable(shape, name):\n",
        "    initial = tf.constant(0.0, shape=shape)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def calculate_model(wrong_model=False):\n",
        "\n",
        "    # Download the dataset\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Define placeholders\n",
        "\n",
        "    # correct labels\n",
        "    y_ = tf.placeholder(tf.float32, shape=(None, NUM_CLASSES), name='y_')\n",
        "    # input data\n",
        "    x = tf.placeholder(tf.float32, shape=(None, NUM_FEATURES), name='x')\n",
        "\n",
        "    # Build the net\n",
        "\n",
        "    hidden_size1 = 500\n",
        "    hidden_size2 = 100\n",
        "\n",
        "    W_fc1 = weight_variable((NUM_FEATURES, hidden_size1), name='W_fct1')\n",
        "    b_fc1 = bias_variable((1, hidden_size1), name='b_fct1')\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)\n",
        "\n",
        "    W_fc2 = weight_variable((hidden_size1, hidden_size2), name='W_fct2')\n",
        "    b_fc2 = bias_variable((1, hidden_size2), name='b_fct2')\n",
        "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
        "\n",
        "    W_fc3 = weight_variable((hidden_size2, NUM_CLASSES), name='W_fct3')\n",
        "    b_fc3 = bias_variable((1, NUM_CLASSES), name='b_fct3')\n",
        "\n",
        "    y = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3, name='OP_TO_RESTORE')\n",
        "\n",
        "    # define the loss function\n",
        "    y_log = tf.log(y)\n",
        "    cross_entropy = tf.reduce_sum(-1 * y_log * y_)\n",
        "\n",
        "    # define Optimizer\n",
        "    Optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Add ops to save and restore all the variables.\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        for i in range(2500):\n",
        "            input_images, correct_predictions = mnist.train.next_batch(TRAIN_BATCH_SIZE)\n",
        "            input_images_test, correct_predictions_test = mnist.test.next_batch(TRAIN_BATCH_SIZE)\n",
        "\n",
        "            if wrong_model == False:\n",
        "                sess.run(Optimizer, feed_dict={x: input_images, y_: correct_predictions})\n",
        "            else:\n",
        "                # Train the 'wrong' model on altered images\n",
        "                input_images_test_altered, correct_predictions_altered = modify_test_set(input_images,\n",
        "                                                            correct_predictions,\n",
        "                                                            modification='do_unbalanced_data',\n",
        "                                                            batch_size=TRAIN_BATCH_SIZE)\n",
        "                sess.run(Optimizer, feed_dict={x: input_images_test_altered, y_: correct_predictions_altered})\n",
        "\n",
        "            if i % 128 == 0:\n",
        "                train_accuracy = sess.run(accuracy, feed_dict={x: input_images, y_: correct_predictions})\n",
        "                print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
        "                test_accuracy = sess.run([accuracy], feed_dict={x: input_images_test, y_: correct_predictions_test})\n",
        "                print(\"Validation accuracy: {}.\".format(test_accuracy))\n",
        "\n",
        "        if wrong_model == False:\n",
        "            saver.save(sess, MODEL_DIR+'/my_test_model_multiclass')\n",
        "        else:\n",
        "            saver.save(sess, MODEL_W_DIR + '/my_test_model_multiclass')\n",
        "\n",
        "\n",
        "def modify_test_set(input_set, label_set, modification='brutally_inverse_pixels', batch_size=TEST_BATCH_SIZE):\n",
        "    if modification == 'brutally_inverse_pixels':\n",
        "        input_set_modified = np.array([1 - x for x in input_set])\n",
        "    if modification == 'divide_brightness':\n",
        "        input_set_modified = np.array([x * 0.50 for x in input_set])\n",
        "    if modification == 'none':\n",
        "        input_set_modified = input_set\n",
        "    if modification == 'do_unbalanced_data':\n",
        "        if random.random() >= 0.05:\n",
        "            input_set_modified = input_set[np.array([np.argmax(x) not in [0, 2, 4, 6, 8] for x in label_set], dtype=bool), :]\n",
        "            label_set_modified = label_set[np.array([np.argmax(x) not in [0, 2, 4, 6, 8] for x in label_set], dtype=bool), :]\n",
        "            return input_set_modified, label_set_modified\n",
        "        else:\n",
        "            input_set_modified = input_set[np.array([np.argmax(x) not in [1, 3, 5, 7, 9] for x in label_set], dtype=bool), :]\n",
        "            label_set_modified = label_set[np.array([np.argmax(x) not in [1, 3, 5, 7, 9] for x in label_set], dtype=bool), :]\n",
        "            return input_set_modified, label_set_modified\n",
        "    if modification == 'remove_high_greys':\n",
        "        thresh = 0.9\n",
        "        input_set_modified = np.array([[pixel if pixel < thresh else 0 for pixel in x] for x in input_set])\n",
        "    if modification == '4_corner_black':\n",
        "        n = 5\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, :n, :n] = 0\n",
        "        input_set_reshaped[:, 28 - n:, :n] = 0\n",
        "        input_set_reshaped[:, :n, 28 - n:] = 0\n",
        "        input_set_reshaped[:, 28 - n:, 28 - n:] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "    if modification == 'center_black':\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, 12:17, 12:17] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "    if modification == 'translate':\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, 4:, :] = input_set_reshaped[:, :24, :]\n",
        "        input_set_reshaped[:, :4, :] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "\n",
        "    return input_set_modified\n",
        "\n",
        "\n",
        "def do_predictions():\n",
        "\n",
        "    # Load the dataset\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Load the previously saved model and graph in sess\n",
        "    sess = tf.Session()\n",
        "    sess_W = tf.Session()\n",
        "    # Let's load meta graph and restore input var x\n",
        "    saver = tf.train.import_meta_graph(MODEL_DIR+'/my_test_model_multiclass.meta')\n",
        "    saver_W = tf.train.import_meta_graph(MODEL_W_DIR+'/my_test_model_multiclass.meta')\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(MODEL_DIR))\n",
        "    saver_W.restore(sess_W, tf.train.latest_checkpoint(MODEL_W_DIR))\n",
        "    graph = tf.get_default_graph()\n",
        "    x = graph.get_tensor_by_name(\"x:0\")\n",
        "\n",
        "    # Predict batches of data\n",
        "    ##EDITABLE##\n",
        "    I = 50\n",
        "    J = 60\n",
        "    STOP = 60\n",
        "    agent = MonitoringAgent(frequency=1, max_buffer_size=100, n_classes=10, agent_id='MNIST_Multiclass', server_IP='127.0.0.1', server_port=8000)\n",
        "    #agent = MonitoringAgent(frequency=1, max_buffer_size=100, agent_id='MNIST_Multiclass_wrong_inputs', server_IP='127.0.0.1', server_port=8000)\n",
        "    #agent = MonitoringAgent(frequency=1, max_buffer_size=100, agent_id='MNIST_Multiclass_wrong_model', server_IP='127.0.0.1', server_port=8000)\n",
        "    agent.run_local_server(n_sockets=5)\n",
        "    ##END EDITABLE##\n",
        "\n",
        "    while True:\n",
        "        for i in range(STOP):\n",
        "            batch_size = randint(1, 20)\n",
        "            input_images_test, correct_predictions_test = mnist.test.next_batch(batch_size)\n",
        "            input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='none', batch_size=batch_size)\n",
        "            #input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='brutally_inverse_pixels', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='divide_brightness', batch_size=batch_size)\n",
        "            # input_images_test_altered, correct_predictions_test = modify_test_set(input_images_test, correct_predictions_test, modification='do_unbalanced_data', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='remove_high_greys', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='4_corner_black', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='center_black', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='translate', batch_size=batch_size)\n",
        "\n",
        "            if i >= I and i <= J and not WRONG_TRAIN:\n",
        "                input_images_test = input_images_test_altered\n",
        "\n",
        "            # result of the softmax : predict_proba matrix of size (batch_size, num_classes)\n",
        "            if i >= I and i <= J and WRONG_TRAIN:\n",
        "                output_proba = sess_W.run('OP_TO_RESTORE:0', feed_dict={x: input_images_test})\n",
        "            else:\n",
        "                output_proba = sess.run('OP_TO_RESTORE:0', feed_dict={x: input_images_test})\n",
        "\n",
        "\n",
        "            ##HERE IS THE HOOK\n",
        "            agent.collect_data(predict_proba_matrix=output_proba, input_matrix=input_images_test, label_matrix=correct_predictions_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba, label_matrix=correct_predictions_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba, input_matrix=input_images_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba)\n",
        "\n",
        "            sleep_time = randint(1, 10)\n",
        "            sleep(sleep_time)\n",
        "\n",
        "\n",
        "def main():\n",
        "    if TRAIN_PHASE:\n",
        "        calculate_model(wrong_model=WRONG_TRAIN)\n",
        "    else:\n",
        "        do_predictions()\n",
        "\n",
        "def save_pickle(result, pickle_name):\n",
        "    \"\"\"Saves an object in pickle object file for future use\"\"\"\n",
        "    with open(os.path.join(WORKING_DIR, pickle_name), 'wb') as file:\n",
        "        pickle.dump(result, file)\n",
        "        return True\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # execute only if run as ********************# IMPORT EXTERNAL LIBRARIES\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from time import sleep\n",
        "from random import randint\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "#### ADD #####\n",
        "# IMPORT AGENT LIB\n",
        "from MLWatcher.agent import MonitoringAgent\n",
        "\n",
        "WORKING_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "MODEL_DIR = os.path.join(WORKING_DIR, \"MODEL_MULTICLASS\")\n",
        "MODEL_W_DIR = os.path.join(WORKING_DIR, \"MODEL_MULTICLASS_WRONG\")\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "        os.mkdir(MODEL_DIR)\n",
        "\n",
        "if not os.path.exists(MODEL_W_DIR):\n",
        "        os.mkdir(MODEL_W_DIR)\n",
        "\n",
        "##EDITABLE##\n",
        "TRAIN_PHASE = True\n",
        "WRONG_TRAIN = False\n",
        "##END EDITABLE##\n",
        "\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "NUM_FEATURES = 784\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "TEST_BATCH_SIZE = 15\n",
        "\n",
        "\n",
        "def weight_variable(shape, name):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def bias_variable(shape, name):\n",
        "    initial = tf.constant(0.0, shape=shape)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def calculate_model(wrong_model=False):\n",
        "\n",
        "    # Download the dataset\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Define placeholders\n",
        "\n",
        "    # correct labels\n",
        "    y_ = tf.placeholder(tf.float32, shape=(None, NUM_CLASSES), name='y_')\n",
        "    # input data\n",
        "    x = tf.placeholder(tf.float32, shape=(None, NUM_FEATURES), name='x')\n",
        "\n",
        "    # Build the net\n",
        "\n",
        "    hidden_size1 = 500\n",
        "    hidden_size2 = 100\n",
        "\n",
        "    W_fc1 = weight_variable((NUM_FEATURES, hidden_size1), name='W_fct1')\n",
        "    b_fc1 = bias_variable((1, hidden_size1), name='b_fct1')\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)\n",
        "\n",
        "    W_fc2 = weight_variable((hidden_size1, hidden_size2), name='W_fct2')\n",
        "    b_fc2 = bias_variable((1, hidden_size2), name='b_fct2')\n",
        "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
        "\n",
        "    W_fc3 = weight_variable((hidden_size2, NUM_CLASSES), name='W_fct3')\n",
        "    b_fc3 = bias_variable((1, NUM_CLASSES), name='b_fct3')\n",
        "\n",
        "    y = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3, name='OP_TO_RESTORE')\n",
        "\n",
        "    # define the loss function\n",
        "    y_log = tf.log(y)\n",
        "    cross_entropy = tf.reduce_sum(-1 * y_log * y_)\n",
        "\n",
        "    # define Optimizer\n",
        "    Optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Add ops to save and restore all the variables.\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        for i in range(2500):\n",
        "            input_images, correct_predictions = mnist.train.next_batch(TRAIN_BATCH_SIZE)\n",
        "            input_images_test, correct_predictions_test = mnist.test.next_batch(TRAIN_BATCH_SIZE)\n",
        "\n",
        "            if wrong_model == False:\n",
        "                sess.run(Optimizer, feed_dict={x: input_images, y_: correct_predictions})\n",
        "            else:\n",
        "                # Train the 'wrong' model on altered images\n",
        "                input_images_test_altered, correct_predictions_altered = modify_test_set(input_images,\n",
        "                                                            correct_predictions,\n",
        "                                                            modification='do_unbalanced_data',\n",
        "                                                            batch_size=TRAIN_BATCH_SIZE)\n",
        "                sess.run(Optimizer, feed_dict={x: input_images_test_altered, y_: correct_predictions_altered})\n",
        "\n",
        "            if i % 128 == 0:\n",
        "                train_accuracy = sess.run(accuracy, feed_dict={x: input_images, y_: correct_predictions})\n",
        "                print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
        "                test_accuracy = sess.run([accuracy], feed_dict={x: input_images_test, y_: correct_predictions_test})\n",
        "                print(\"Validation accuracy: {}.\".format(test_accuracy))\n",
        "\n",
        "        if wrong_model == False:\n",
        "            saver.save(sess, MODEL_DIR+'/my_test_model_multiclass')\n",
        "        else:\n",
        "            saver.save(sess, MODEL_W_DIR + '/my_test_model_multiclass')\n",
        "\n",
        "\n",
        "def modify_test_set(input_set, label_set, modification='brutally_inverse_pixels', batch_size=TEST_BATCH_SIZE):\n",
        "    if modification == 'brutally_inverse_pixels':\n",
        "        input_set_modified = np.array([1 - x for x in input_set])\n",
        "    if modification == 'divide_brightness':\n",
        "        input_set_modified = np.array([x * 0.50 for x in input_set])\n",
        "    if modification == 'none':\n",
        "        input_set_modified = input_set\n",
        "    if modification == 'do_unbalanced_data':\n",
        "        if random.random() >= 0.05:\n",
        "            input_set_modified = input_set[np.array([np.argmax(x) not in [0, 2, 4, 6, 8] for x in label_set], dtype=bool), :]\n",
        "            label_set_modified = label_set[np.array([np.argmax(x) not in [0, 2, 4, 6, 8] for x in label_set], dtype=bool), :]\n",
        "            return input_set_modified, label_set_modified\n",
        "        else:\n",
        "            input_set_modified = input_set[np.array([np.argmax(x) not in [1, 3, 5, 7, 9] for x in label_set], dtype=bool), :]\n",
        "            label_set_modified = label_set[np.array([np.argmax(x) not in [1, 3, 5, 7, 9] for x in label_set], dtype=bool), :]\n",
        "            return input_set_modified, label_set_modified\n",
        "    if modification == 'remove_high_greys':\n",
        "        thresh = 0.9\n",
        "        input_set_modified = np.array([[pixel if pixel < thresh else 0 for pixel in x] for x in input_set])\n",
        "    if modification == '4_corner_black':\n",
        "        n = 5\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, :n, :n] = 0\n",
        "        input_set_reshaped[:, 28 - n:, :n] = 0\n",
        "        input_set_reshaped[:, :n, 28 - n:] = 0\n",
        "        input_set_reshaped[:, 28 - n:, 28 - n:] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "    if modification == 'center_black':\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, 12:17, 12:17] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "    if modification == 'translate':\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, 4:, :] = input_set_reshaped[:, :24, :]\n",
        "        input_set_reshaped[:, :4, :] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "\n",
        "    return input_set_modified\n",
        "\n",
        "\n",
        "def do_predictions():\n",
        "\n",
        "    # Load the dataset\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Load the previously saved model and graph in sess\n",
        "    sess = tf.Session()\n",
        "    sess_W = tf.Session()\n",
        "    # Let's load meta graph and restore input var x\n",
        "    saver = tf.train.import_meta_graph(MODEL_DIR+'/my_test_model_multiclass.meta')\n",
        "    saver_W = tf.train.import_meta_graph(MODEL_W_DIR+'/my_test_model_multiclass.meta')\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(MODEL_DIR))\n",
        "    saver_W.restore(sess_W, tf.train.latest_checkpoint(MODEL_W_DIR))\n",
        "    graph = tf.get_default_graph()\n",
        "    x = graph.get_tensor_by_name(\"x:0\")\n",
        "\n",
        "    # Predict batches of data\n",
        "    ##EDITABLE##\n",
        "    I = 50\n",
        "    J = 60\n",
        "    STOP = 60\n",
        "    agent = MonitoringAgent(frequency=1, max_buffer_size=100, n_classes=10, agent_id='MNIST_Multiclass', server_IP='127.0.0.1', server_port=8000)\n",
        "    #agent = MonitoringAgent(frequency=1, max_buffer_size=100, agent_id='MNIST_Multiclass_wrong_inputs', server_IP='127.0.0.1', server_port=8000)\n",
        "    #agent = MonitoringAgent(frequency=1, max_buffer_size=100, agent_id='MNIST_Multiclass_wrong_model', server_IP='127.0.0.1', server_port=8000)\n",
        "    agent.run_local_server(n_sockets=5)\n",
        "    ##END EDITABLE##\n",
        "\n",
        "    while True:\n",
        "        for i in range(STOP):\n",
        "            batch_size = randint(1, 20)\n",
        "            input_images_test, correct_predictions_test = mnist.test.next_batch(batch_size)\n",
        "            input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='none', batch_size=batch_size)\n",
        "            #input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='brutally_inverse_pixels', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='divide_brightness', batch_size=batch_size)\n",
        "            # input_images_test_altered, correct_predictions_test = modify_test_set(input_images_test, correct_predictions_test, modification='do_unbalanced_data', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='remove_high_greys', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='4_corner_black', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='center_black', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='translate', batch_size=batch_size)\n",
        "\n",
        "            if i >= I and i <= J and not WRONG_TRAIN:\n",
        "                input_images_test = input_images_test_altered\n",
        "\n",
        "            # result of the softmax : predict_proba matrix of size (batch_size, num_classes)\n",
        "            if i >= I and i <= J and WRONG_TRAIN:\n",
        "                output_proba = sess_W.run('OP_TO_RESTORE:0', feed_dict={x: input_images_test})\n",
        "            else:\n",
        "                output_proba = sess.run('OP_TO_RESTORE:0', feed_dict={x: input_images_test})\n",
        "\n",
        "\n",
        "            ##HERE IS THE HOOK\n",
        "            agent.collect_data(predict_proba_matrix=output_proba, input_matrix=input_images_test, label_matrix=correct_predictions_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba, label_matrix=correct_predictions_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba, input_matrix=input_images_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba)\n",
        "\n",
        "            sleep_time = randint(1, 10)\n",
        "            sleep(sleep_time)\n",
        "\n",
        "\n",
        "def main():\n",
        "    if TRAIN_PHASE:\n",
        "        calculate_model(wrong_model=WRONG_TRAIN)\n",
        "    else:\n",
        "        do_predictions()\n",
        "\n",
        "def save_pickle(result, pickle_name):\n",
        "    \"\"\"Saves an object in pickle object file for future use\"\"\"\n",
        "    with open(os.path.join(WORKING_DIR, pickle_name), 'wb') as file:\n",
        "        pickle.dump(result, file)\n",
        "        return True\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # execute only if run as a script\n",
        "    main()# IMPORT EXTERNAL LIBRARIES\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from time import sleep\n",
        "from random import randint\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "#### ADD #####\n",
        "# IMPORT AGENT LIB\n",
        "from MLWatcher.agent import MonitoringAgent\n",
        "\n",
        "WORKING_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "MODEL_DIR = os.path.join(WORKING_DIR, \"MODEL_MULTICLASS\")\n",
        "MODEL_W_DIR = os.path.join(WORKING_DIR, \"MODEL_MULTICLASS_WRONG\")\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "        os.mkdir(MODEL_DIR)\n",
        "\n",
        "if not os.path.exists(MODEL_W_DIR):\n",
        "        os.mkdir(MODEL_W_DIR)\n",
        "\n",
        "##EDITABLE##\n",
        "TRAIN_PHASE = True\n",
        "WRONG_TRAIN = False\n",
        "##END EDITABLE##\n",
        "\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "NUM_FEATURES = 784\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "TEST_BATCH_SIZE = 15\n",
        "\n",
        "\n",
        "def weight_variable(shape, name):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def bias_variable(shape, name):\n",
        "    initial = tf.constant(0.0, shape=shape)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def calculate_model(wrong_model=False):\n",
        "\n",
        "    # Download the dataset\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Define placeholders\n",
        "\n",
        "    # correct labels\n",
        "    y_ = tf.placeholder(tf.float32, shape=(None, NUM_CLASSES), name='y_')\n",
        "    # input data\n",
        "    x = tf.placeholder(tf.float32, shape=(None, NUM_FEATURES), name='x')\n",
        "\n",
        "    # Build the net\n",
        "\n",
        "    hidden_size1 = 500\n",
        "    hidden_size2 = 100\n",
        "\n",
        "    W_fc1 = weight_variable((NUM_FEATURES, hidden_size1), name='W_fct1')\n",
        "    b_fc1 = bias_variable((1, hidden_size1), name='b_fct1')\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)\n",
        "\n",
        "    W_fc2 = weight_variable((hidden_size1, hidden_size2), name='W_fct2')\n",
        "    b_fc2 = bias_variable((1, hidden_size2), name='b_fct2')\n",
        "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
        "\n",
        "    W_fc3 = weight_variable((hidden_size2, NUM_CLASSES), name='W_fct3')\n",
        "    b_fc3 = bias_variable((1, NUM_CLASSES), name='b_fct3')\n",
        "\n",
        "    y = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3, name='OP_TO_RESTORE')\n",
        "\n",
        "    # define the loss function\n",
        "    y_log = tf.log(y)\n",
        "    cross_entropy = tf.reduce_sum(-1 * y_log * y_)\n",
        "\n",
        "    # define Optimizer\n",
        "    Optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Add ops to save and restore all the variables.\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        for i in range(2500):\n",
        "            input_images, correct_predictions = mnist.train.next_batch(TRAIN_BATCH_SIZE)\n",
        "            input_images_test, correct_predictions_test = mnist.test.next_batch(TRAIN_BATCH_SIZE)\n",
        "\n",
        "            if wrong_model == False:\n",
        "                sess.run(Optimizer, feed_dict={x: input_images, y_: correct_predictions})\n",
        "            else:\n",
        "                # Train the 'wrong' model on altered images\n",
        "                input_images_test_altered, correct_predictions_altered = modify_test_set(input_images,\n",
        "                                                            correct_predictions,\n",
        "                                                            modification='do_unbalanced_data',\n",
        "                                                            batch_size=TRAIN_BATCH_SIZE)\n",
        "                sess.run(Optimizer, feed_dict={x: input_images_test_altered, y_: correct_predictions_altered})\n",
        "\n",
        "            if i % 128 == 0:\n",
        "                train_accuracy = sess.run(accuracy, feed_dict={x: input_images, y_: correct_predictions})\n",
        "                print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
        "                test_accuracy = sess.run([accuracy], feed_dict={x: input_images_test, y_: correct_predictions_test})\n",
        "                print(\"Validation accuracy: {}.\".format(test_accuracy))\n",
        "\n",
        "        if wrong_model == False:\n",
        "            saver.save(sess, MODEL_DIR+'/my_test_model_multiclass')\n",
        "        else:\n",
        "            saver.save(sess, MODEL_W_DIR + '/my_test_model_multiclass')\n",
        "\n",
        "\n",
        "def modify_test_set(input_set, label_set, modification='brutally_inverse_pixels', batch_size=TEST_BATCH_SIZE):\n",
        "    if modification == 'brutally_inverse_pixels':\n",
        "        input_set_modified = np.array([1 - x for x in input_set])\n",
        "    if modification == 'divide_brightness':\n",
        "        input_set_modified = np.array([x * 0.50 for x in input_set])\n",
        "    if modification == 'none':\n",
        "        input_set_modified = input_set\n",
        "    if modification == 'do_unbalanced_data':\n",
        "        if random.random() >= 0.05:\n",
        "            input_set_modified = input_set[np.array([np.argmax(x) not in [0, 2, 4, 6, 8] for x in label_set], dtype=bool), :]\n",
        "            label_set_modified = label_set[np.array([np.argmax(x) not in [0, 2, 4, 6, 8] for x in label_set], dtype=bool), :]\n",
        "            return input_set_modified, label_set_modified\n",
        "        else:\n",
        "            input_set_modified = input_set[np.array([np.argmax(x) not in [1, 3, 5, 7, 9] for x in label_set], dtype=bool), :]\n",
        "            label_set_modified = label_set[np.array([np.argmax(x) not in [1, 3, 5, 7, 9] for x in label_set], dtype=bool), :]\n",
        "            return input_set_modified, label_set_modified\n",
        "    if modification == 'remove_high_greys':\n",
        "        thresh = 0.9\n",
        "        input_set_modified = np.array([[pixel if pixel < thresh else 0 for pixel in x] for x in input_set])\n",
        "    if modification == '4_corner_black':\n",
        "        n = 5\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, :n, :n] = 0\n",
        "        input_set_reshaped[:, 28 - n:, :n] = 0\n",
        "        input_set_reshaped[:, :n, 28 - n:] = 0\n",
        "        input_set_reshaped[:, 28 - n:, 28 - n:] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "    if modification == 'center_black':\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, 12:17, 12:17] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "    if modification == 'translate':\n",
        "        input_set = np.copy(input_set)\n",
        "        input_set_reshaped = np.reshape(input_set, (batch_size, 28, 28))\n",
        "        input_set_reshaped[:, 4:, :] = input_set_reshaped[:, :24, :]\n",
        "        input_set_reshaped[:, :4, :] = 0\n",
        "        input_set_modified = np.reshape(input_set_reshaped, (batch_size, 28 * 28))\n",
        "\n",
        "    return input_set_modified\n",
        "\n",
        "\n",
        "def do_predictions():\n",
        "\n",
        "    # Load the dataset\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Load the previously saved model and graph in sess\n",
        "    sess = tf.Session()\n",
        "    sess_W = tf.Session()\n",
        "    # Let's load meta graph and restore input var x\n",
        "    saver = tf.train.import_meta_graph(MODEL_DIR+'/my_test_model_multiclass.meta')\n",
        "    saver_W = tf.train.import_meta_graph(MODEL_W_DIR+'/my_test_model_multiclass.meta')\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(MODEL_DIR))\n",
        "    saver_W.restore(sess_W, tf.train.latest_checkpoint(MODEL_W_DIR))\n",
        "    graph = tf.get_default_graph()\n",
        "    x = graph.get_tensor_by_name(\"x:0\")\n",
        "\n",
        "    # Predict batches of data\n",
        "    ##EDITABLE##\n",
        "    I = 50\n",
        "    J = 60\n",
        "    STOP = 60\n",
        "    agent = MonitoringAgent(frequency=1, max_buffer_size=100, n_classes=10, agent_id='MNIST_Multiclass', server_IP='127.0.0.1', server_port=8000)\n",
        "    #agent = MonitoringAgent(frequency=1, max_buffer_size=100, agent_id='MNIST_Multiclass_wrong_inputs', server_IP='127.0.0.1', server_port=8000)\n",
        "    #agent = MonitoringAgent(frequency=1, max_buffer_size=100, agent_id='MNIST_Multiclass_wrong_model', server_IP='127.0.0.1', server_port=8000)\n",
        "    agent.run_local_server(n_sockets=5)\n",
        "    ##END EDITABLE##\n",
        "\n",
        "    while True:\n",
        "        for i in range(STOP):\n",
        "            batch_size = randint(1, 20)\n",
        "            input_images_test, correct_predictions_test = mnist.test.next_batch(batch_size)\n",
        "            input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='none', batch_size=batch_size)\n",
        "            #input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='brutally_inverse_pixels', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='divide_brightness', batch_size=batch_size)\n",
        "            # input_images_test_altered, correct_predictions_test = modify_test_set(input_images_test, correct_predictions_test, modification='do_unbalanced_data', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='remove_high_greys', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='4_corner_black', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='center_black', batch_size=batch_size)\n",
        "            # input_images_test_altered = modify_test_set(input_images_test, correct_predictions_test, modification='translate', batch_size=batch_size)\n",
        "\n",
        "            if i >= I and i <= J and not WRONG_TRAIN:\n",
        "                input_images_test = input_images_test_altered\n",
        "\n",
        "            # result of the softmax : predict_proba matrix of size (batch_size, num_classes)\n",
        "            if i >= I and i <= J and WRONG_TRAIN:\n",
        "                output_proba = sess_W.run('OP_TO_RESTORE:0', feed_dict={x: input_images_test})\n",
        "            else:\n",
        "                output_proba = sess.run('OP_TO_RESTORE:0', feed_dict={x: input_images_test})\n",
        "\n",
        "\n",
        "            ##HERE IS THE HOOK\n",
        "            agent.collect_data(predict_proba_matrix=output_proba, input_matrix=input_images_test, label_matrix=correct_predictions_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba, label_matrix=correct_predictions_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba, input_matrix=input_images_test)\n",
        "            #agent.collect_data(predict_proba_matrix=output_proba)\n",
        "\n",
        "            sleep_time = randint(1, 10)\n",
        "            sleep(sleep_time)\n",
        "\n",
        "\n",
        "def main():\n",
        "    if TRAIN_PHASE:\n",
        "        calculate_model(wrong_model=WRONG_TRAIN)\n",
        "    else:\n",
        "        do_predictions()\n",
        "\n",
        "def save_pickle(result, pickle_name):\n",
        "    \"\"\"Saves an object in pickle object file for future use\"\"\"\n",
        "    with open(os.path.join(WORKING_DIR, pickle_name), 'wb') as file:\n",
        "        pickle.dump(result, file)\n",
        "        return True\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # execute only if run as a script\n",
        "    main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d7b219138752>\"\u001b[0;36m, line \u001b[0;32m237\u001b[0m\n\u001b[0;31m    import os\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    }
  ]
}